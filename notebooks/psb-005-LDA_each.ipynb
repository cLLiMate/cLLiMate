{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA for each dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import gensim\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_parquet(\"../data/processed/news-consolidated-v1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>embedding</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abc</td>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>[0.42550426721572876, 0.5782315135002136, 0.09...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abc</td>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>epa still trying to recover chemical clean up ...</td>\n",
       "      <td>[0.33238619565963745, -0.3517177700996399, 0.5...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>abc</td>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>expressions of interest sought to build livestock</td>\n",
       "      <td>[0.4847770035266876, 0.10000099241733551, -0.0...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>abc</td>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>iraq to pay for own rebuilding white house</td>\n",
       "      <td>[0.4847399592399597, 0.20435450971126556, 0.19...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>abc</td>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>meeting to focus on broken hill water woes</td>\n",
       "      <td>[0.3507457375526428, 0.43837735056877136, -0.0...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id source       date                                           headline  \\\n",
       "0   0    abc 2003-02-19     a g calls for infrastructure protection summit   \n",
       "1   1    abc 2003-02-19  epa still trying to recover chemical clean up ...   \n",
       "2   2    abc 2003-02-19  expressions of interest sought to build livestock   \n",
       "3   3    abc 2003-02-19         iraq to pay for own rebuilding white house   \n",
       "4   4    abc 2003-02-19         meeting to focus on broken hill water woes   \n",
       "\n",
       "                                           embedding   url  \n",
       "0  [0.42550426721572876, 0.5782315135002136, 0.09...  null  \n",
       "1  [0.33238619565963745, -0.3517177700996399, 0.5...  null  \n",
       "2  [0.4847770035266876, 0.10000099241733551, -0.0...  null  \n",
       "3  [0.4847399592399597, 0.20435450971126556, 0.19...  null  \n",
       "4  [0.3507457375526428, 0.43837735056877136, -0.0...  null  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "\n",
    "\n",
    "def preProcessData(dataframe):\n",
    "    headlines = dataframe[\"headline\"].tolist()\n",
    "\n",
    "    # Same process as psb-000-mini_LDA.ipynb\n",
    "\n",
    "    # Tokenize the documents.\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    for idx in range(len(headlines)):\n",
    "        # Remove punctuation and lowercase the documents.\n",
    "        headlines[idx] = tokenizer.tokenize(headlines[idx])\n",
    "        headlines[idx] = [w.lower() for w in headlines[idx] if len(w) > 1]\n",
    "\n",
    "    # Remove stopwords and lemmatize the documents\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for idx in range(len(headlines)):\n",
    "        headlines[idx] = [w for w in headlines[idx] if not w in stop_words]\n",
    "        headlines[idx] = [lemmatizer.lemmatize(token) for token in headlines[idx]]\n",
    "\n",
    "    # Add bigrams\n",
    "    percent_freq = 0.02\n",
    "\n",
    "    bigram = Phrases(headlines, min_count=percent_freq * len(headlines))\n",
    "    for idx in range(len(headlines)):\n",
    "        for token in bigram[headlines[idx]]:\n",
    "            if \"_\" in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                headlines[idx].append(token)\n",
    "\n",
    "    dictionary = Dictionary(headlines)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in headlines]\n",
    "\n",
    "    return dictionary, corpus, headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "\n",
    "climateDB_data = preProcessData(total_data[total_data[\"source\"] == \"climate-db\"])\n",
    "abcNews_data = preProcessData(total_data[total_data[\"source\"] == \"abc\"])\n",
    "natureNews_data = preProcessData(total_data[total_data[\"source\"] == \"nature\"])\n",
    "newsapi_data = preProcessData(total_data[total_data[\"source\"] == \"news-api\"])\n",
    "\n",
    "total_data_processed = preProcessData(total_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m temp \u001b[39m=\u001b[39m total_data_processed[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m id2word \u001b[39m=\u001b[39m total_data_processed[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mid2token\n\u001b[0;32m---> 12\u001b[0m model \u001b[39m=\u001b[39m LdaModel(\n\u001b[1;32m     13\u001b[0m     corpus\u001b[39m=\u001b[39;49mtotal_data_processed[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     14\u001b[0m     id2word\u001b[39m=\u001b[39;49mid2word,\n\u001b[1;32m     15\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m     16\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     eta\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     iterations\u001b[39m=\u001b[39;49miterations,\n\u001b[1;32m     19\u001b[0m     num_topics\u001b[39m=\u001b[39;49mnum_topics,\n\u001b[1;32m     20\u001b[0m     passes\u001b[39m=\u001b[39;49mpasses,\n\u001b[1;32m     21\u001b[0m     eval_every\u001b[39m=\u001b[39;49meval_every,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    519\u001b[0m use_numpy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(corpus, chunks_as_numpy\u001b[39m=\u001b[39;49muse_numpy)\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    524\u001b[0m     msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrained \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py:1006\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1003\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPROGRESS: pass \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, at document #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1004\u001b[0m         pass_, chunk_no \u001b[39m*\u001b[39m chunksize \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(chunk), lencorpus\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[0;32m-> 1006\u001b[0m     gammat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_estep(chunk, other)\n\u001b[1;32m   1008\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_alpha:\n\u001b[1;32m   1009\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_alpha(gammat, rho())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py:768\u001b[0m, in \u001b[0;36mLdaModel.do_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n\u001b[0;32m--> 768\u001b[0m gamma, sstats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(chunk, collect_sstats\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    769\u001b[0m state\u001b[39m.\u001b[39msstats \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sstats\n\u001b[1;32m    770\u001b[0m state\u001b[39m.\u001b[39mnumdocs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m gamma\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]  \u001b[39m# avoids calling len(chunk) on a generator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gensim/models/ldamodel.py:721\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    719\u001b[0m gammad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m+\u001b[39m expElogthetad \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mdot(cts \u001b[39m/\u001b[39m phinorm, expElogbetad\u001b[39m.\u001b[39mT)\n\u001b[1;32m    720\u001b[0m Elogthetad \u001b[39m=\u001b[39m dirichlet_expectation(gammad)\n\u001b[0;32m--> 721\u001b[0m expElogthetad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mexp(Elogthetad)\n\u001b[1;32m    722\u001b[0m phinorm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[39m+\u001b[39m epsilon\n\u001b[1;32m    723\u001b[0m \u001b[39m# If gamma hasn't changed much, we're done.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 1000\n",
    "passes = 50\n",
    "iterations = 500\n",
    "eval_every = None\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = total_data_processed[0][0]\n",
    "id2word = total_data_processed[0].id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=total_data_processed[1],\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\",\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "climateDB_top_topics = model.top_topics(climateDB_data[1])\n",
    "abcNews_top_topics = model.top_topics(abcNews_data[1])\n",
    "natureNews_top_topics = model.top_topics(natureNews_data[1])\n",
    "newsapi_top_topics = model.top_topics(newsapi_data[1])\n",
    "total_data_top_topics = model.top_topics(total_data_processed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus, total_data, limit=40, start=2, step=6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding top topics by running Model, saving outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"../data/models/psb-005-LDA_each_V1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0.30361468, climate), (0.24107258, change), ...</td>\n",
       "      <td>-8.193566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(0.050904073, cost), (0.047834627, forest), (...</td>\n",
       "      <td>-8.966744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(0.092042096, action), (0.0888979, plan), (0....</td>\n",
       "      <td>-9.682544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(0.16531192, say), (0.100059465, sea), (0.048...</td>\n",
       "      <td>-10.080515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(0.08891194, risk), (0.061359864, health), (0...</td>\n",
       "      <td>-10.164523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(0.100586936, world), (0.093343854, study), (...</td>\n",
       "      <td>-10.181912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(0.06676394, fire), (0.056533743, end), (0.05...</td>\n",
       "      <td>-10.288619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(0.060831357, people), (0.055473045, life), (...</td>\n",
       "      <td>-10.326685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(0.0988149, year), (0.062059756, pacific), (0...</td>\n",
       "      <td>-10.481036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(0.10420526, based), (0.060167354, north), (0...</td>\n",
       "      <td>-10.945895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[(0.097808726, report), (0.06925717, scale), (...</td>\n",
       "      <td>-10.963617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[(0.18988277, water), (0.07046749, level), (0....</td>\n",
       "      <td>-11.255773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[(0.12395881, state), (0.07278994, drought), (...</td>\n",
       "      <td>-11.368157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[(0.13024546, u), (0.117327265, australia), (0...</td>\n",
       "      <td>-11.776390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[(0.079270385, show), (0.07259205, assessment)...</td>\n",
       "      <td>-12.121038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[(0.192072, ocean), (0.16088104, impact), (0.0...</td>\n",
       "      <td>-12.331340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[(0.11570506, china), (0.11377102, energy), (0...</td>\n",
       "      <td>-12.491033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[(0.23371375, global), (0.108388215, could), (...</td>\n",
       "      <td>-12.504188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[(0.1546687, crisis), (0.09254239, system), (0...</td>\n",
       "      <td>-12.607580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[(0.17956746, high), (0.06843031, policy), (0....</td>\n",
       "      <td>-12.692229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topics      score\n",
       "0   [(0.30361468, climate), (0.24107258, change), ...  -8.193566\n",
       "1   [(0.050904073, cost), (0.047834627, forest), (...  -8.966744\n",
       "2   [(0.092042096, action), (0.0888979, plan), (0....  -9.682544\n",
       "3   [(0.16531192, say), (0.100059465, sea), (0.048... -10.080515\n",
       "4   [(0.08891194, risk), (0.061359864, health), (0... -10.164523\n",
       "5   [(0.100586936, world), (0.093343854, study), (... -10.181912\n",
       "6   [(0.06676394, fire), (0.056533743, end), (0.05... -10.288619\n",
       "7   [(0.060831357, people), (0.055473045, life), (... -10.326685\n",
       "8   [(0.0988149, year), (0.062059756, pacific), (0... -10.481036\n",
       "9   [(0.10420526, based), (0.060167354, north), (0... -10.945895\n",
       "10  [(0.097808726, report), (0.06925717, scale), (... -10.963617\n",
       "11  [(0.18988277, water), (0.07046749, level), (0.... -11.255773\n",
       "12  [(0.12395881, state), (0.07278994, drought), (... -11.368157\n",
       "13  [(0.13024546, u), (0.117327265, australia), (0... -11.776390\n",
       "14  [(0.079270385, show), (0.07259205, assessment)... -12.121038\n",
       "15  [(0.192072, ocean), (0.16088104, impact), (0.0... -12.331340\n",
       "16  [(0.11570506, china), (0.11377102, energy), (0... -12.491033\n",
       "17  [(0.23371375, global), (0.108388215, could), (... -12.504188\n",
       "18  [(0.1546687, crisis), (0.09254239, system), (0... -12.607580\n",
       "19  [(0.17956746, high), (0.06843031, policy), (0.... -12.692229"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame(total_data_top_topics, columns=[\"topics\", \"score\"])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics as parquet\n",
    "climateDB_topics = pd.DataFrame(climateDB_top_topics, columns=[\"topics\", \"score\"])\n",
    "climateDB_topics.to_csv(\n",
    "    \"../data/intermediate/top_topics/psb-005-LDA_each_v1-climateDB.csv\"\n",
    ")\n",
    "\n",
    "abcNews_topics = pd.DataFrame(abcNews_top_topics, columns=[\"topics\", \"score\"])\n",
    "abcNews_topics.to_csv(\"../data/intermediate/top_topics/psb-005-LDA_each_v1-abcNews.csv\")\n",
    "\n",
    "natureNews_topics = pd.DataFrame(natureNews_top_topics, columns=[\"topics\", \"score\"])\n",
    "natureNews_topics.to_csv(\n",
    "    \"../data/intermediate/top_topics/psb-005-LDA_each_v1-natureNews.csv\"\n",
    ")\n",
    "\n",
    "newsapi_topics = pd.DataFrame(newsapi_top_topics, columns=[\"topics\", \"score\"])\n",
    "newsapi_topics.to_csv(\"../data/intermediate/top_topics/psb-005-LDA_each_v1-newsapi.csv\")\n",
    "\n",
    "total_data_top_topics = pd.DataFrame(total_data_top_topics, columns=[\"topics\", \"score\"])\n",
    "total_data_top_topics.to_csv(\n",
    "    \"../data/intermediate/top_topics/psb-005-LDA_each_v1-total_data.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a3870566885d8713b456c7b616bb4ec819c6611b841c812a534b80b621588a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
